# ğŸ§  Reinforcement Learning-Based Path Planning for Robotic Brain Surgery Simulation

> Autonomous surgical needle path planning using Q-Learning and DQN in a PyBullet physics simulation with a Franka Panda robotic arm.

**University of Maryland, College Park** | Yaxita Amin & K. Manasanjani

---

## ğŸ“– Overview

This project presents a reinforcement learning approach for autonomous path planning in robotic brain surgery simulation. An AI agent learns to navigate a surgical needle through complex 3D brain vasculature â€” avoiding blood vessels while reaching tumor targets â€” using Q-Learning and Deep Q-Network (DQN) algorithms.

Key highlights:
- **98â€“100% training success rate** with tabular Q-Learning over 3,000 episodes
- **80â€“100% generalization** to unseen tumor targets
- **0% vessel collision rate** vs. 40% for straight-line approaches
- **â‰¥4mm safety margin** guaranteed from all blood vessels
- Full integration with a Franka Panda robotic arm via Inverse Kinematics

---

## ğŸ¥ Demo Videos

---

## ğŸ—ï¸ System Architecture

![Brain Surgery Visualization](_- visual selection (1).png)

---

## âš™ï¸ Installation

### Prerequisites
- Python 3.10+
- Ubuntu (recommended) or macOS

### Setup

```bash
# Clone the repository
git clone https://github.com/<your-username>/rl-brain-surgery-path-planning.git
cd rl-brain-surgery-path-planning

# Create a virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Dependencies

```txt
pybullet
trimesh
scipy
numpy
torch
matplotlib
```

---

## ğŸš€ Usage

### Run Q-Learning (Path Planning + PyBullet Simulation)

```bash
python final13.py
```

### Run DQN Agent

```bash
python brain_surgery_dqn.py
```

---

## ğŸ§ª Results

### Training Performance

| Episode | Q-Learning | DQN    | Epsilon |
|---------|------------|--------|---------|
| 500     | 84.8%      | 95.2%  | ~0.22   |
| 1000    | 98.2%      | 98.0%  | ~0.05   |
| 2000    | 99.6%      | 97.2%  | ~0.01   |
| 3000    | 99.6%      | 98.8%  | 0.01    |

### Q-Learning vs DQN

| Metric           | Q-Learning    | DQN         |
|------------------|---------------|-------------|
| Training Time    | ~2 min        | ~15 min     |
| Training Success | 94.6%         | 89.2%       |
| Test Success     | 80%           | 75%         |
| Path Quality     | 1.1Ã—          | 1.15Ã—       |
| Memory Usage     | 4,003 states  | 128KB model |
| Inference Time   | <1ms          | ~5ms        |

### Path Planning Method Comparison

| Metric          | Dijkstra/A* | RRT     | Potential Fields | Q-Learning  |
|-----------------|-------------|---------|------------------|-------------|
| Computation     | 30â€“35s      | 5â€“10s   | 2â€“5s             | ~2 min train|
| Path Quality    | 1.0â€“1.3Ã—    | 1.4â€“1.6Ã—| 1.2â€“1.4Ã—         | ~1.1Ã—       |
| Success Rate    | 60â€“65%      | 70%     | 65â€“70%           | **98â€“100%** |
| Vessel Safety   | Occasional  | Generally| >4mm*           | **Always â‰¥4mm** |
| Reproducibility | High        | Low     | Medium           | High        |
| Learning        | None        | None    | None             | **Yes**     |

---

## ğŸ¤– RL Formulation

### State Space
3D voxel grid position (2mm resolution) relative to tumor location.

### Action Space
6 discrete moves: `{+X, âˆ’X, +Y, âˆ’Y, +Z, âˆ’Z}` (2mm per step)

### Reward Function
```
R(s, a, s') = R_goal + R_collision + R_timeout + R_shaping
```
- `R_goal = +100` â€” reaching the tumor
- `R_collision = -100` â€” vessel proximity < 4mm
- `R_timeout = -50` â€” exceeding 100 steps
- `R_shaping = Î”d` â€” distance reduction to target

### Hyperparameters
| Parameter | Value |
|-----------|-------|
| Learning rate Î± | 0.15 |
| Discount factor Î³ | 0.95 |
| Epsilon (start â†’ end) | 1.0 â†’ 0.01 |
| Epsilon decay | 0.997 |
| Max steps/episode | 100 |
| Training episodes | 3,000 |

---

## ğŸ¦¾ Robotic Arm Integration

- **Robot**: Franka Emika Panda (7 DOF, simulated in PyBullet)
- **IK Solver**: PyBullet damped least-squares with joint limit handling
- **IK success rate**: 98%
- **End-effector accuracy**: <0.5mm positioning error
- **Average execution time**: 8.5 seconds per path

---

## ğŸ“ Project Structure

```
brain_surgery_docker/
â”œâ”€â”€ data/                       # STL models & brain vasculature data (~7.5MB)
â”œâ”€â”€ final13.py                  # Q-Learning main script (path planning + simulation)
â”œâ”€â”€ brain_surgery_dqn.py        # Deep Q-Network implementation
â”œâ”€â”€ Dockerfile                  # Docker container configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ³ Docker

A `Dockerfile` is included for containerized, reproducible execution.

### Build the Image

```bash
docker build -t brain-surgery-rl .
```

### Run the Container

```bash
docker run --rm brain-surgery-rl
```

> **Note:** PyBullet GUI visualization requires passing through a display. On Linux, use:
> ```bash
> docker run --rm -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix brain-surgery-rl
> ```

---

## âš ï¸ Limitations

- Static environment (no tissue deformation modeling)
- Discrete action space limits path smoothness
- Single-needle, straight-segment paths only
- Simulated environment differs from real surgical conditions

---

## ğŸ”­ Future Work

- Extend to continuous state/action spaces using actor-critic methods
- Incorporate curved needle steering for challenging targets
- Multi-objective optimization (path length, clearance, energy)
- Dynamic replanning with real-time MRI/CT intraoperative feedback
- Sim-to-real transfer for physical Franka Panda deployment
- Training on diverse anatomical models for patient-agnostic planning

---

## ğŸ“„ Citation

```bibtex
@article{amin2024rl_surgery,
  title={Reinforcement Learning-Based Path Planning for Robotic Brain Surgery Simulation},
  author={Amin, Yaxita and Manasanjani, K.},
  institution={University of Maryland, College Park},
  year={2024}
}
```

---

## ğŸ™ Acknowledgments

We thank **Dr. Jerry Wu** for guidance throughout this project, and teaching assistants **Siddhant** and **Aswin** for valuable feedback during development.

---

## ğŸ“œ License

This project is intended for educational and pre-operative planning research purposes only. Not for clinical use.
